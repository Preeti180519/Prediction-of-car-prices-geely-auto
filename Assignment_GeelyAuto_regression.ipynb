{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### A Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts. \n",
    "### For that we need to build a multiple linear regression model for the prediction of car prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Reading and understanding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns',50)\n",
    "pd.set_option('display.max_rows',50)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'CarPrice_Assignment.csv' does not exist: b'CarPrice_Assignment.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-713f7609f94b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Check the head of the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"CarPrice_Assignment.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'CarPrice_Assignment.csv' does not exist: b'CarPrice_Assignment.csv'"
     ]
    }
   ],
   "source": [
    "# Check the head of the dataset\n",
    "df=pd.read_csv(\"CarPrice_Assignment.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(['car_ID'],axis=1) #dropping car id column as it is not required\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum() # find the sum of null values in all the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "insight: There are no null values in any columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Car company name from CarName column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting car company name from CarName column\n",
    "df['CarName']=df['CarName'].str.split(' ',expand=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CarName'].unique() #finding unique carname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some company name which is spelled incorrectly<br>\n",
    "      mazda = maxda <br>\n",
    "      Nissan = nissan <br>\n",
    "      porsche = porcshce <br>\n",
    "      toyota = toyouta <br>\n",
    "      vokswagen = volkswagen = vw <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing incorrect company name by correct one\n",
    "df['CarName']=df['CarName'].replace({'maxda':'mazda','nissan':'Nissan','porcshce':'porsche','toyouta':'toyota','vw':'volkswagen','vokswagen':'volkswagen'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CarName'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorizing symboling column<br>\n",
    "Here Symboling is the assigned insurance risk rating,value of +3 indicates that the auto is risky, -3 that it is probably pretty safe.\n",
    "Let's make the category clear by classifying based on risk:\n",
    "\n",
    "-ve symboling as safe<br>\n",
    "0, 1 as moderate<br>\n",
    "2,3 as risky<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using map function to assign category\n",
    "df['symboling'] = df['symboling'].map({-2:'safe',-1:'safe',0:'moderate',1:'moderate',2:'risky',3:'risky'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the list of all the categorical values\n",
    "categorical_variables=list(df.columns[df.dtypes=='object'])\n",
    "categorical_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA for detailed analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots for all the categorical values\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.subplot(5,2,1)\n",
    "sns.countplot('symboling',data=df)\n",
    "plt.subplot(5,2,2)\n",
    "sns.countplot('fueltype',data=df)\n",
    "plt.subplot(5,2,3)\n",
    "sns.countplot('aspiration',data=df)\n",
    "plt.subplot(5,2,4)\n",
    "sns.countplot('doornumber',data=df)\n",
    "plt.subplot(5,2,5)\n",
    "sns.countplot('carbody',data=df)\n",
    "plt.subplot(5,2,6)\n",
    "sns.countplot('drivewheel',data=df)\n",
    "plt.subplot(5,2,7)\n",
    "sns.countplot('enginelocation',data=df)\n",
    "plt.subplot(5,2,8)\n",
    "sns.countplot('enginetype',data=df)\n",
    "plt.subplot(5,2,9)\n",
    "sns.countplot('cylindernumber',data=df)\n",
    "plt.subplot(5,2,10)\n",
    "sns.countplot('fuelsystem',data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "insights : We can identify some of the car features that are predominant in the US Automobile Market.These features are:<br>\n",
    "\n",
    "-symboling: moderate (0,1)<br>\n",
    "-Carbody: Sedan<br>\n",
    "-fueltype: gas<br>\n",
    "-aspiration: standard<br>\n",
    "-doornumbers: four<br>\n",
    "-drivewheel: forward<br>\n",
    "-engine location: front<br>\n",
    "-engine type: ohc<br>\n",
    "-cylinderNumber: four<br>\n",
    "-fuelSystem: mpfi<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting carname to find which company has maximum price\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.countplot('CarName',data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "insight:From above graph it is clearly visible that Toyota has dominant market followed by Nissan and mazda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heat map to find correlation between diffrent numeric variables\n",
    "plt.figure(figsize = (20, 20))\n",
    "ax=sns.heatmap(df.corr(),annot=True,cmap=\"rainbow\")\n",
    "x,y=ax.get_ylim()\n",
    "ax=ax.set_ylim(x+0.5,y-0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "insight : From above plot we can clearly see there are many varaibles which have good correlation with price like enginesize,horsepower,curbweight,carwidth.<br>\n",
    "we can also observe some multicollinearity among varaibles like enginesize and horsepower,wheelbase and carlength,citympg and highwaympg etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Visualising the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### paiplot of all numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_values=['wheelbase','carlength','carwidth','carheight','curbweight','enginesize','boreratio','stroke','compressionratio','horsepower','peakrpm','citympg','highwaympg','price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50,80))\n",
    "g=sns.pairplot(df,x_vars=[\"carlength\",\"wheelbase\",\"carwidth\",\"carheight\",\"curbweight\",\"enginesize\",\"boreratio\",],y_vars=[\"price\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50,80))\n",
    "g=sns.pairplot(df,x_vars=[\"stroke\",\"compressionratio\",\"horsepower\",\"peakrpm\",\"citympg\",\"highwaympg\"],y_vars=[\"price\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see enginesize, horsepower and compression ratio variables to have outliers .<br>\n",
    "Enginesize,curbweight,horsepower has high positive correlation with price whereas citympg,highwaympg is negatively correlated with price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### outliers treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['horsepower','curbweight','enginesize']].quantile([0.01, .96])) #fining the percentile values\n",
    "print(df[['compressionratio']].quantile([0.01, .90]))#fining the percentile values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treating Outilers in price of cars\n",
    "df['horsepower'][np.abs(df['horsepower'] > 182.00)]= 182.00 # clippomg variables at 96 percentile\n",
    "df['horsepower'][np.abs(df['horsepower'] > 3657.80)]= 3657.80 # clippomg variables at 96 percentile\n",
    "df['enginesize'][np.abs(df['enginesize'] > 209.00)]= 209.00 # clippomg variables at  96 percentile\n",
    "df['compressionratio'][np.abs(df['compressionratio'] > 10.94)]= 10.94 # clippomg variables at 90 percentile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### boxplot for all categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_values=['Symboling','carCompany','fueltype','aspiration','doornumber','carbody','drivewheel','enginelocation','enginetype','cylindernumber','fuelsystem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting all categorical variables to see relation with price\n",
    "plt.figure(figsize=(15,40))\n",
    "plt.subplot(10,1,1)\n",
    "sns.boxplot(x='fueltype',y='price',data=df)\n",
    "plt.subplot(10,1,2)\n",
    "sns.boxplot(x='aspiration',y='price',data=df)\n",
    "plt.subplot(10,1,3)\n",
    "sns.boxplot(x='doornumber',y='price',data=df)\n",
    "plt.subplot(10,1,4)\n",
    "sns.boxplot(x='carbody',y='price',data=df)\n",
    "plt.subplot(10,1,5)\n",
    "sns.boxplot(x='drivewheel',y='price',data=df)\n",
    "plt.subplot(10,1,6)\n",
    "sns.boxplot(x='enginelocation',y='price',data=df)\n",
    "plt.subplot(10,1,7)\n",
    "sns.boxplot(x='enginetype',y='price',data=df)\n",
    "plt.subplot(10,1,8)\n",
    "sns.boxplot(x='cylindernumber',y='price',data=df)\n",
    "plt.subplot(10,1,9)\n",
    "sns.boxplot(x='fuelsystem',y='price',data=df)\n",
    "plt.subplot(10,1,10)\n",
    "sns.boxplot(x='symboling',y='price',data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "    \n",
    "Although not significant but still the fuel type seems to have an effect on the pricing of the cars. <br>\n",
    "Enginelocation (rear) and aspiration(turbo) has a visible effect on the pricing of the car.<br>\n",
    "The price of real wheel drive is significantly higher that other drivewheel options.<br>\n",
    "Cylindernumber and engine type also seem to regulate the price of cars.<br>\n",
    "Hardtop and convertables cars are priced higher than other body types available. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting Carcompany with respect to price\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.boxplot(x='CarName',y='price',data=df)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean price for top companies:<br>\n",
    "jaguar    :      34600.00<br>\n",
    "buick     :     33647.00<br>\n",
    "porsche   :     31400.50<br>\n",
    "bmw       :    26118.75<br>\n",
    "volvo     :     18063.18<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Creating new variables(Derived metrics)\n",
    "This will be helpful to remove correlated variables.<br>\n",
    "\n",
    "Its observed that there is a high correlation between carlength, wheelbase, car width, car weight and city/highway mpg's.<br>\n",
    "Lets create new variables from these to try reducing the multicolinearlity.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new variable carLWratio\n",
    "df['carLWratio'] = df.carlength/df.carwidth\n",
    "# Creating new variable carWHratio\n",
    "df['carWHratio'] = df.carwidth/df.carheight\n",
    "# Creating new variable PWratio\n",
    "df['PWratio'] = df.horsepower/df.curbweight\n",
    "# Creating new variable HCmpgratio\n",
    "df['HCmpgratio'] = df.highwaympg/df.citympg\n",
    "## droping the orignal variables\n",
    "df.drop(['carlength','carwidth','carheight','highwaympg','citympg'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we saw that the company brand value is determinig the pricing of the car.\n",
    "So We will segment the car companies based on the mean company price as:\n",
    "\n",
    "lower if company mean price is below 10,000<br>\n",
    "mid if company mean price is above 10,000 and below 20,000<br>\n",
    "higher if company mean price is above 20,000<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding mean price with respect to car company name\n",
    "df.groupby(['CarName']).price.mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#segment car comapnies based on mean price range\n",
    "company_segment_dict = {\n",
    "    'cheverolet' : 'lower',\n",
    "    'dodge' : 'lower',\n",
    "    'plymouth' : 'lower',\n",
    "    'honda' : 'lower',\n",
    "    'subaru' : 'lower',\n",
    "    'isuzu' : 'lower',\n",
    "    'mitsubishi' : 'lower',\n",
    "    'renault' : 'lower',\n",
    "    'toyota' : 'lower',\n",
    "    'volkswagen' : 'mid',\n",
    "    'nissan' : 'mid',\n",
    "    'mazda' : 'mid',\n",
    "    'saab' : 'mid',\n",
    "    'peugeot' : 'mid',\n",
    "    'alfa-romero' : 'mid',\n",
    "    'mercury' : 'mid',\n",
    "    'audi' : 'mid',\n",
    "    'volvo' : 'mid',\n",
    "    'bmw' : 'higher',\n",
    "    'buick' : 'higher',\n",
    "    'porsche' : 'higher',\n",
    "    'jaguar' : 'higher',\n",
    "    }\n",
    "df['company_segment'] = df['CarName'].map(company_segment_dict)\n",
    "# Dropping the orignal carName variable\n",
    "df.drop('CarName',axis=1,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('company_segment').price.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating dummy variables for all categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique values for fueltype\n",
    "print(\"unique values in fueltype\")\n",
    "print(df['fueltype'].unique())\n",
    "print('\\n')\n",
    "\n",
    "# unique values for aspiration\n",
    "print(\"Unique values in aspiration\")\n",
    "print(df['aspiration'].unique())\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# unique values for doornumber\n",
    "print(\"Unique values in doornumber\")\n",
    "print(df['doornumber'].unique())\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# unique values for carbody\n",
    "print(\"Unique values in carbody\")\n",
    "print(df['carbody'].unique())\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# unique values for drivewheel\n",
    "print(\"Unique values in drivewheel\")\n",
    "print(df['drivewheel'].unique())\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# unique values for enginelocation\n",
    "print(\"Unique values in enginelocation\")\n",
    "print(df['enginelocation'].unique())\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# unique values for enginetype\n",
    "print(\"Unique values in enginetype\")\n",
    "print(df['enginetype'].unique())\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# unique values for cylindernumber\n",
    "print(\"Unique values in cylindernumber\")\n",
    "print(df['cylindernumber'].unique())\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# unique values for fuelsystem\n",
    "print(\"Unique values in fuelsystem\")\n",
    "print(df['fuelsystem'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dummy variables\n",
    "status=pd.get_dummies(df[['company_segment','carbody','drivewheel','enginetype','cylindernumber','fuelsystem','fueltype','aspiration','doornumber','enginelocation','symboling']],drop_first=True)\n",
    "status.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([df,status],axis=1) #adding all dummy variavles with original dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping variables whose dummy has already been created as those colums will not be required\n",
    "df=df.drop(['company_segment','carbody','drivewheel','enginetype','cylindernumber','fuelsystem','fueltype','aspiration','doornumber','enginelocation','symboling'],axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. splitting the data into training and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the first basic step for regression is performing a train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(0)\n",
    "df_train,df_test=train_test_split(df,train_size=0.7,test_size=0.3,random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Rescaling the Features using min_max scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varlist=['price','peakrpm','horsepower','compressionratio','stroke','boreratio','enginesize','curbweight','wheelbase','carLWratio','carWHratio','PWratio','HCmpgratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply scaler() to all the columns except the 'yes-no' and 'dummy' variables\n",
    "df_train[varlist]=scaler.fit_transform(df_train[varlist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the correlation coefficients to see which variables are highly correlated\n",
    "df_train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from above correlation enginesize seems to be correlated to price the most. Let's see a pairplot for area vs price.\n",
    "plt.figure(figsize=[6,6])\n",
    "plt.scatter(df_train.enginesize, df_train.price)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing into x and y sets for the model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=df_train.pop('price')\n",
    "x_train=df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  5. Building a linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing RFE and LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running RFE with output numnber of varibales 15\n",
    "lm=LinearRegression()\n",
    "lm.fit(x_train,y_train)\n",
    "rfe=RFE(lm,15)\n",
    "rfe=rfe.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(x_train.columns,rfe.support_,rfe.ranking_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col=x_train.columns[rfe.support_]\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.columns[~rfe.support_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building model using statsmodel, for the detailed statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_rfe=x_train[col]\n",
    "x_train_rfe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a regression line through the training data using `statsmodels`.\n",
    "In `statsmodels`, we need to explicitly fit a constant using `sm.add_constant(X)` because if we don't perform this step, `statsmodels` fits a regression line passing through the origin, by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a constant\n",
    "x_train_rfe=sm.add_constant(x_train_rfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a first fitted model\n",
    "lr=sm.OLS(y_train,x_train_rfe).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lr.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the VIF values of the feature variables.\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n",
    "vif=pd.DataFrame()\n",
    "vif['Features']=x_train_rfe.columns\n",
    "vif['VIF']=[variance_inflation_factor(x_train_rfe.values,i)for i in range(x_train_rfe.shape[1])]\n",
    "vif['VIF']=round(vif['VIF'],2)\n",
    "vif=vif.sort_values(by='VIF',ascending=False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_new=x_train_rfe.drop(['cylindernumber_twelve'],axis=1) # dropping enginetype_rotor because it has very high p value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm \n",
    "x_train_new=sm.add_constant(x_train_new)\n",
    "# Create a second fitted model\n",
    "lr1=sm.OLS(y_train,x_train_new).fit()\n",
    "print(lr1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n",
    "vif=pd.DataFrame()\n",
    "vif['Features']=x_train_new.columns\n",
    "vif['VIF']=[variance_inflation_factor(x_train_new.values,i)for i in range(x_train_new.shape[1])]\n",
    "vif['VIF']=round(vif['VIF'],2)\n",
    "vif=vif.sort_values(by='VIF',ascending=False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_new1=x_train_new.drop(['carWHratio'],axis=1) #dropping because of high p value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_new1=sm.add_constant(x_train_new1)\n",
    "# Create a third fitted model\n",
    "lr2=sm.OLS(y_train,x_train_new1).fit()\n",
    "print(lr2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n",
    "vif=pd.DataFrame()\n",
    "vif['Features']=x_train_new1.columns\n",
    "vif['VIF']=[variance_inflation_factor(x_train_new1.values,i)for i in range(x_train_new1.shape[1])]\n",
    "vif['VIF']=round(vif['VIF'],2)\n",
    "vif=vif.sort_values(by='VIF',ascending=False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_new2=x_train_new1.drop(['horsepower'],axis=1)#dropping beacuse of high VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_new2=sm.add_constant(x_train_new2)\n",
    "# Create a fourth fitted model\n",
    "lr3=sm.OLS(y_train,x_train_new2).fit()\n",
    "print(lr3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n",
    "vif=pd.DataFrame()\n",
    "vif['Features']=x_train_new2.columns\n",
    "vif['VIF']=[variance_inflation_factor(x_train_new2.values,i)for i in range(x_train_new2.shape[1])]\n",
    "vif['VIF']=round(vif['VIF'],2)\n",
    "vif=vif.sort_values(by='VIF',ascending=False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_new3=x_train_new2.drop(['carLWratio'],axis=1)#dropping because of high p value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_new3=sm.add_constant(x_train_new3)\n",
    "# Create a fifth fitted model\n",
    "lr4=sm.OLS(y_train,x_train_new3).fit()\n",
    "print(lr4.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n",
    "vif=pd.DataFrame()\n",
    "vif['Features']=x_train_new3.columns\n",
    "vif['VIF']=[variance_inflation_factor(x_train_new3.values,i)for i in range(x_train_new3.shape[1])]\n",
    "vif['VIF']=round(vif['VIF'],2)\n",
    "vif=vif.sort_values(by='VIF',ascending=False)\n",
    "vif                                                                                                                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_new4=x_train_new3.drop(['enginetype_dohcv'],axis=1)# dropping p value greater than 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_new4=sm.add_constant(x_train_new4)\n",
    "# Create a sixth fitted model\n",
    "lr5=sm.OLS(y_train,x_train_new4).fit()\n",
    "print(lr5.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n",
    "vif=pd.DataFrame()\n",
    "vif['Features']=x_train_new4.columns\n",
    "vif['VIF']=[variance_inflation_factor(x_train_new4.values,i)for i in range(x_train_new4.shape[1])]\n",
    "vif['VIF']=round(vif['VIF'],2)\n",
    "vif=vif.sort_values(by='VIF',ascending=False)\n",
    "vif "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_new5=x_train_new4.drop(['const'],axis=1) #dropping constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Residual Analysis of the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_price=lr5.predict(x_train_new4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.distplot((y_train-y_train_price),bins=20)\n",
    "plt.suptitle('Error Terms', fontsize = 20)                  \n",
    "plt.xlabel('y_test', fontsize=18)                          # X-label\n",
    "plt.ylabel('y_pred', fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Making Predictions Using the Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the scaling on the test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varlist=['price','peakrpm','horsepower','compressionratio','stroke','boreratio','enginesize','curbweight','wheelbase','carLWratio','carWHratio','PWratio','HCmpgratio']\n",
    "df_test[varlist]=scaler.transform(df_test[varlist])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing into x_test and y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=df_test.pop('price')\n",
    "x_test=df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's use our model to make predictions.\n",
    "# Creating X_test_new dataframe by dropping variables from X_test\n",
    "\n",
    "import statsmodels.api as sm\n",
    "x_test_new=x_test[x_train_new5.columns]\n",
    "\n",
    "# Adding a constant variable \n",
    "x_test_new=sm.add_constant(x_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "y_pred=lr5.predict(x_test_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting y_test and y_pred to understand the spread.\n",
    "fig=plt.figure()\n",
    "plt.scatter(y_test,y_pred)\n",
    "fig.suptitle('y pred vs y test',fontsize=20)\n",
    "plt.xlabel('y_test',fontsize=20)\n",
    "plt.ylabel('y_pred',fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r_squared = r2_score(y_test, y_pred)\n",
    "print('Mean_Squared_Error :' ,mse)\n",
    "print('r_square_value :',r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that the equation of our best fitted line is:\n",
    "\n",
    "$ price = 0.259  \\times  wheelbase + 0.66  \\times  enginesize - 0.202 \\times stroke + 0.127 \\times PWratio + 0.082 \\times enginetype_ohc - 0.1044 \\times enginetype_ohcv - 0.229 \\times cylindernumber_five - 0.29 \\times cylindernumber_four+ 0.268 \\times enginelocation_rear-0.243 \\times cylindernumber_six $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a low p-value and low VIF, below variables do describe the price of the automobiles to a good extent.<br>\n",
    "\n",
    "wheelbase<br>\n",
    "enginesize<br>\n",
    "PW ratio<br>\n",
    "enginetype_ohc<br>\n",
    "enginelocation_rear<br>\n",
    "cylindernumber_five<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R squared for train model is : 0.88 <br>\n",
    "R squared for test model is : 0.86 <br>\n",
    "since there is only difference of 0.02 between test and train model so overall it is a decent model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the model even better we have a couple of options:<br>\n",
    "\n",
    "1)Add new features <br>\n",
    "2)choosing another set of variables to get a more normal distribution of error terms<br>\n",
    "3)Build a non-linear model<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
